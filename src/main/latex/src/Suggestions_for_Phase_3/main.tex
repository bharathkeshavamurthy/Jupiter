\documentclass{article}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{ulem}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{hhline}
\usepackage{yfonts,color}
\usepackage{soul,xcolor}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage{bm}
\usepackage{url}
\usepackage{array}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{balance}
\usepackage{epsfig,epstopdf}
\usepackage{booktabs}
\usepackage{courier}
\usepackage{subfigure}
\usepackage{pseudocode}
\usepackage{enumerate}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[utf8]{inputenc}
\title{Suggestions for Improvements to the BAM-Wireless radio node design for SC2 Phase 3}
\author{Bharath Keshavamurthy}
\date{March 2019}
\begin{document}
\maketitle
\section{Work in Progress}
\begin{itemize}
    \item \textit{Updates to the source code in order to incorporate new scoring rules in Phase-3}: Differentiated Flows and Bonus Thresholds
    \item \textit{Throughput-Optimal Cross-Layer Design}: Formulate a Network Utility Maximization (NUM) problem and solve it using Lagrangian Duality methods and heuristic algorithms in order to arrive at throughput-optimal solutions for MCS Adaptation, Channel Allocation, Routing, and Weighted-Flow Scheduling
    \begin{itemize}
        \item A Utility Maximization Problem with constraints corresponding to protocols in each layer of the cognitive radio node's protocol stack where the utility function can be a simple log-utility function as seen in proportional fairness problems, is formulated.
        \item Constraints include non-interference with the incumbent, minimum throughput requirements (for each allotted flow), flow-routing constraints (node-balance, non-negative rates, and capacity constraints), non-interference among secondary nodes, and constraints for preventing cyclic paths in routes
        \item Decompose the NUM problem into sub-problems by removing the coupling among constraints and tackle each sub-problem separately
        \item Solve these problems using sub-gradient methods with gradient-projection - some solutions are discussed in [\ref{1}] and [\ref{2}]. They include,
        \begin{itemize}
            \item Weighted Flow Scheduling - Back-pressure scheduler (assign bandwidth to a flow with the highest utility metric in terms its point value and queue differential measure)
            \item Routing - Choose the most stable routes instead of simple minimum-hop routing
            \item Channel Allocation - Each node calculates, for each of its outgoing links, the belief that a channel is idle/free in its interference region. Additional requirements will be included for satisfying the scenario gates - non-interference with the incumbent and access only to the allowed portion of the spectrum. The nodes will follow CSMA in the MAC layer with an exponential back-off procedure based on this channel availability metric and the queue differential measure. The higher the availability of the channel or the higher the back-pressure on the link, the more aggressive is the CSMA back-off policy.
        \end{itemize}
    \end{itemize}
    \item Assuming a Markovian Correlation among channels and across time wherein the model parameters are learnt over time using an online Parameter Estimation Algorithm, we can formulate a POMDP that provides the channel availability metrics outlined in the previous point. Since the belief space is going to be huge and the observation space continuous, we can use Approximate Value Iteration Algorithms like the PBVI (Point-Based Value Iteration) Algorithm or the PERSEUS Algorithm (Randomized Point-Based Value Iteration) in order to extract the channel availability metrics during the course of interaction of the POMDP agent with the radio environment and combine it with the queue differential measure to come up with the optimal policy.
\end{itemize}
\section{Possible Extensions}
There is the problem of System Dynamics involved in the POMDP formulation. Since there are a lot of moving parts to the radio environment we're operating in, recent research detailed in [\ref{3}] and [\ref{4}] suggest that Re-Learning OR Re-Training gives us significantly better performance than our current approach of assuming orthogonality among channels and operating a reactive strategy.
\\For instance, a proactive strategy with re-training is shown to be an effective solution as opposed to a Reactive Whittle-Index based strategy in [\ref{3}]. We can take two approaches to re-training:
\begin{itemize}
    \item We can develop extensions to the work outlined in [\ref{4}] in order to re-learn the most relevant belief states and then use those in our Approximate Value Iteration Algorithm to solve for an optimal policy.
    \begin{itemize}
        \item We can re-sample the set of reachable belief points using our most recent policy when the accumulated reward experiences a significant drop.
        \item We can then use this new set of reachable belief points to solve for a new optimal policy using the PERSEUS algorithm ("Backup" until all belief points in the reachable set have been sampled and Update the Value Function for these beliefs based on the chosen policy tree vector).
    \end{itemize}
    \item Another possible approach would be to employ Adaptive Deep Q-Networks. Adaptive DQNs turn out to be great tools to solve for an optimal policy in highly-dynamic environments where the system statistics are unknown.
    \begin{itemize}
        \item Reference [\ref{3}] uses the Deep Q-Learning with Experience Replay Algorithm to design the DQN.
        \begin{itemize}
            \item Design: 2-layer Neural Network with 200 neurons and a ReLU activation function at each neuron
            \item An $\epsilon$-greedy policy is employed to select actions (channel combinations) in a given state. The interaction record - \textit{(state, action, observation, reward, next-state)} is stored in a "Replay Memory" and a random set of these historic records are used to compute the loss function.
            \item The weights of the DQN are updated using the stochastic gradient-descent algorithm
        \end{itemize}
        \item This DQN is used in conjunction with a re-training policy which involves evaluation of the accumulated reward of the current policy and simply comparing it with a threshold in order to trigger re-training to find a new good policy.
    \end{itemize}
\end{itemize}
\section{References}
\begin{enumerate}
    \item "\href{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7859326&isnumber=7859429}{\textcolor{blue}{Cross-Layer Optimization and Protocol Analysis for Cognitive Ad Hoc Communications}}"\label{1}
    \item "\href{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6881740&isnumber=7180482}{\textcolor{blue}{Throughput-Optimal Cross-Layer Design for Cognitive Radio Ad Hoc Networks}}"\label{2}
    \item "\href{https://ieeexplore.ieee.org/document/8303773}{\textcolor{blue}{Deep Reinforcement Learning for Dynamic Multi-Channel Access in Wireless Networks}}"\label{3}
    \item "\href{https://arxiv.org/pdf/1109.2145.pdf}{\textcolor{blue}{Perseus: Randomized Point-based Value Iteration for POMDPs}}"\label{4}
    \item \href{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1665000&isnumber=34851}{\textcolor{blue}{A tutorial on cross-layer optimization in wireless networks}}"
    \item \href{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4118456&isnumber=4118453}{\textcolor{blue}{Layering as Optimization Decomposition: A Mathematical Theory of Network Architectures}}"
\end{enumerate}
\end{document}
